# -*- coding: utf-8 -*-
"""GNN_(Prac).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11heAHQ_5pCbrteO9Tl-2yUwdZev842AB
"""

from re import X
import numpy as np

m = 10**2
n = 10**2
Q = 10**3
Q_test = 10**3
def SampleGenerator(m,n):
    return np.random.binomial(1,1/(m),(m,n))

def InputsGenerator(n):
    return np.random.normal(np.zeros(n),((np.sqrt(0.5/n)))*np.ones(n))

def TrainingSetGenerator(A,q):
    [m,n] = A.shape
    X = InputsGenerator(n)
    Y = (A @ X + InputsGenerator(m))  # Changed from H to A here
    for i in range(q-1):
        X = np.column_stack((X,InputsGenerator(n)))
        Y = np.column_stack((Y,A@X[:,-1] + InputsGenerator(m)))
    X = X.T
    Y = Y.T
    return X,Y

### for the case of sign model generator function ###
def TrainingSetGeneratorSign(A,q):
    [m,n] = A.shape
    X = InputsGenerator(n)
    Y = (A @ X + InputsGenerator(m))  # Changed from H to A here
    for i in range(q-1):
        X = np.column_stack((X,InputsGenerator(n)))
        Y = np.column_stack((Y,A@X[:,-1] + InputsGenerator(m)))
    X = X.T
    Y = np.sign(Y.T)
    return X,Y

A = SampleGenerator(m,n)
T_x, T_y = TrainingSetGenerator(A,Q)
T_x_prime, T_y_prime = TrainingSetGeneratorSign(A,Q)

H = T_y.T @ T_x @ np.linalg.inv(T_x.T @ T_x)

Test_x,Test_y = TrainingSetGenerator(A,Q_test)
test_pred = Test_x @ H
train_pred = T_x_prime @ H
train_mse = np.mean((train_pred - T_y)**2)
print(f"The MSE of the training data is : {train_mse:.4f}")
test_mse = np.mean((test_pred - Test_y)**2)
print(f"The MSE of the unseen data is : {test_mse:.4f}")

### Stochastic gradient descent ###

### defining the gradient function initially
import matplotlib.pyplot as plt
m = 10**2
def Grad(X,Y,H):
  matricial = (1/X.shape[0])*(-Y.T + H @ X.T) @ X
  return matricial
n = 10**2
m = 10**2
Q = 10**3

b_size = 10 #batchsize
stepsize = 1 #stepsize , e.g. epsilon
A = SampleGenerator(m,n)
T_x,T_y = TrainingSetGenerator(A,Q)
Test_x,Test_y = TrainingSetGeneratorSign(A,Q)

H = np.zeros((m,n))
error_train = []
error_test = []

for i in range(Q):
  permutation = np.random.permutation(Q)
  mb_x = np.array([T_x[index] for index in permutation[0:b_size]])
  mb_y = np.array([T_y[index] for index in permutation[0:b_size]])
  grad = Grad(mb_x,mb_y,H)
  H = H - stepsize*grad

  error_train = error_train+[np.linalg.norm(T_y.T - H@T_x.T)**2/Q]
  error_test  = error_test+[np.linalg.norm(Test_y.T - H@Test_x.T)**2/Q]

plt.rcParams['figure.figsize'] = (36,6)
plt.plot(range(len(error_train)),error_train,color = 'r', marker = 'o') #Plot
plt.plot(range(len(error_train)),error_test,'*b') #Plot

plt.title('MSE using Gradient Descent')
plt.xlabel('Iterations')
plt.ylabel('MSE')
plt.savefig('MSE_computing_grad.pdf', bbox_inches='tight', dpi=150)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

def SampleGenerator(m,n):
  return np.random.binomial(1,1/(m),(m,n))
def InputsGenerator(n):
  return np.random.normal(np.zeros(n),((np.sqrt(0.5/n)))*np.ones(n))
def TrainingSetGenerator(A,q):
  [m,n] = A.shape
  X = InputsGenerator(n)
  Y = (A @ X + InputsGenerator(m))

  for i in range(q-1):
    X = np.column_stack((X,InputsGenerator(n)))
    Y = np.column_stack((Y , A @ X[:,-1] + InputsGenerator(m)))

  X = X.T
  Y = Y.T
  return X,Y
####for the case of sign model of the sample generator function we have :-
def TrainingSetGeneratorSign(A,q):
  [m,n] = A.shape
  X = InputsGenerator(n)
  # Changed InputsGenerator(n) to InputsGenerator(m) based on the first cell
  Y = (A @ X + InputsGenerator(m))

  for i in range(q-1):
    X = np.column_stack((X , InputsGenerator(n)))
    # Changed A @ X + InputsGenerator(m) to A @ X[:,-1] + InputsGenerator(m) based on the first cell
    Y = np.column_stack((Y , A @ X[:,-1] + InputsGenerator(m)))

  X = X.T
  Y = np.sign(Y.T)
  return X,Y

n = 10**2
m = 10**2
Q = 10**3
A = SampleGenerator(m,n) # Call SampleGenerator with m, n

T_x ,T_y = TrainingSetGenerator(A,Q)
T_x_prime,T_y_prime = TrainingSetGenerator(A,Q)

H = T_y.T @ T_x @ np.linalg.inv(T_x.T @ T_x)

def Grad(X,Y,H):
  matricial = (1/X.shape[0])*(-Y.T + H @ X.T) @ X
  return matricial

b_size = 10 # batch size
stepsize = 1 # esilon
A = SampleGenerator(m,n)
T_x,T_y = TrainingSetGenerator(A,Q)
test_X,test_Y = TrainingSetGenerator(A,Q)

H = np.zeros((m,n))
error_train = []
error_test = []

for i in range(Q):
  permutation = np.random.permutation(Q)
  mb_x = T_x[permutation[0:b_size]]
  mb_y = T_y[permutation[0:b_size]]

  grad = Grad(mb_x,mb_y,H)
  H = H - stepsize*grad

  error_train.append(np.linalg.norm(T_y.T - H @ T_x.T)**2/Q)
  error_test.append(np.linalg.norm(test_Y.T - H @ test_X.T)**2/Q)


plt.rcParams['figure.figsize'] = (36,6)
plt.plot(range(len(error_train)),error_train,color = 'blue', marker = 'o', label='Training Error')
plt.plot(range(len(error_test)),error_test,'*g', label='Test Error')

plt.title('MSE using Gradient Descent')
plt.xlabel('Iterations')
plt.ylabel('MSE')
plt.legend()
plt.savefig('MSE_computing_grad.pdf', bbox_inches='tight', dpi=150)
plt.show()

"""#**LAB-2**
* **Source Localization**
"""

import numpy as np
#Stochastic Block Matrix for inter and intra community
def sbm(n,c,p_intra,p_inter):
  community = np.repeat(list(range(c)), np.ceil(n/c))
  community = community[0:n]
  ### making it a column vector
  community = np.expand_dims(community,axis=1)
  intra = community == community.T
  inter = np.logical_not(intra)
  random = np.random.random((n,n))
  ### SBM graph is symmetric hence one needs to assign weights to the upper half
  tri = np.tri(n , k=-1)
  graph = np.zeros((n,n))
  graph[np.logical_and.reduce([tri,intra,random<p_intra])] = 1
  graph[np.logical_and.reduce([tri,inter,random<p_inter])] = 1
  graph +=graph.T ### making adjacency matrix symmetric
  return graph
###Provided requirements N=50 nodes;C=5 communities and P_ci_ci=0.6 and P_cj_cj=0.2
S = sbm(n=50,c=5,p_intra=0.6,p_inter=0.2)
def normalize_gso(gso):
  eig_val,eig_vec = np.linalg.eig(gso)
  return gso/np.max(np.abs(eig_val))

S_new = normalize_gso(S)
S_new

def generate_diffusion(gso,n_samples,n_sources):
  n = gso.shape[0] #getting the number of nodes
  z = np.zeros((n_samples,n,5,1)) #initializing the tensor
  for i in range(n_samples):
    sources = np.random.choice(n,n_sources,replace=False)
    #### defining the output Z0
    z[i,sources,0,0] = np.random.uniform(0,10,n_sources)
  mu = np.zeros(n) ##noise mean
  sigma = np.eye(n)*1e-3 ##noise variance
  for t in range(4):
    noise = np.random.multivariate_normal(mu,sigma,n_samples)
    z[:,:,t+1] = gso @ z[:,:,t] + np.expand_dims(noise,-1)
  z = z.transpose((0,2,1,3))
  return z.squeeze()
z = generate_diffusion(S_new,1000,10)

def data_from_diffusion(z):
  z = np.random.permutation(z)
  y = np.expand_dims(z[:,0,:],1)
  x = np.zeros(y.shape)
  for i,sample in enumerate(z):
    x[i] = sample[4]
  return x.squeeze(),y.squeeze()
# x,y = data_from_diffusion(z)
def split_data(x,y,splits=(2000,100)):
  splits = np.cumsum([0] + list(splits))
  splits = (splits * x.shape[0]/splits[-1]).astype(int)
  return ((x[splits[i]:splits[i+1]], y[splits[i]:splits[i+1]]) for i in range(len(splits) - 1))

x,y = data_from_diffusion(z)
trainData,testData = split_data(x,y,(2000,100))
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]

import torch
xTrain = torch.tensor(xTrain)
yTrain = torch.tensor(yTrain)
xTest = torch.tensor(xTest)
yTest = torch.tensor(yTest)

xTrain

"""**Graph Filter Function**"""

import torch
def FilterFunction(h,S,x):
  K = h.shape[0] #Number of filter taps
  B = x.shape[0] #batch size
  N = x.shape[1] #number of nodes

  x = x.reshape([B,1,N])
  S = S.reshape([1,N,N])
  z = x
  for k in range(1,K):
    x = torch.matmul(x,S)
    xS = x.reshape([B,1,N])
    z = torch.cat((z,xS), dim =1)
  y = torch.matmul(z.permute(0,2,1).reshape([B,N,K]),h)
  return y

import torch.nn as nn
import math
class GraphFilter(nn.Module):
  def __init__(self,gso,k):
    super().__init__()
    self.gso = torch.tensor(gso)
    self.n = gso.shape[0]
    self.k = k
    self.weight = nn.Parameter(torch.randn(self.k))
  def reset_paramters(self):
    stdv = 1./math.sqrt(self.k)
    self.weight.data.uniform(-stdv,stdv)
  def forward(self,x):
    return FilterFunction(self.weight,self.gso,x)

graphfilter = GraphFilter(S,8)

"""**Repracticing**"""

import numpy as np
#ctochastic block matrix
def sbm(n=50, c=5, p_intra=0.6, p_inter=0.2):
  community = np.repeat(list(range(c)),np.ceil(n/c))
  community = community[0:n] ## making sure community size is n
  community = np.expand_dims(community,axis=1) ## making it a column vector
  intra = community == community.T
  inter = np.logical_not(intra)
  random = np.random.random((n,n))
  tri = np.tri(n,k=-1)
  graph = np.zeros((n,n)) ### initializing adjacency matrix
  graph[np.logical_and.reduce([tri,intra, random < p_intra])] = 1
  graph[np.logical_and.reduce([tri,inter, random < p_inter])] = 1
  graph += graph.T
  return graph
def normalize_gso(gso):
  eig_val,eig_vec = np.linalg.eig(gso)
  return gso/np.max(np.abs(eig_val))
def generate_diffusion(gso,n_samples=2100,n_sources=10):
  n = gso.shape[0] ##number of nodes
  ## stroing the samples
  z = np.zeros((n_samples, n, 11, 1))
  for i in range(n_samples):
    sources = np.random.choice(n, n_sources, replace=False)
    z[i, sources, 0, 0] = np.random.uniform(0,10)
  mu = np.zeros(n) ##noise mean
  sigma = np.eye(n) * 1e-3
  for t in range(10):
    ## noise generation
    noise = np.random.multivariate_normal(mu,sigma,n_samples)
    ## generate z_t
    z[:, :, t+1] = gso @ z[:,:, t] + np.expand_dims(noise, -1)
  z = z.transpose((0,2,1,3))
  return z.squeeze()
def data_from_diffusion(z):
  z = np.random.permutation(z)
  y = np.expand_dims(z[:, 0, :], 1)
  x = np.zeros(y.shape)
  for i, sample in enumerate(z):
    x[i] = sample[4]
  return x.squeeze(), y.squeeze()
def split_data(x,y, splits=(2000,100)):
  splits = np.cumsum([0] + list(splits))
  splits = (splits * x.shape[0]/splits[-1]).astype(int)
  return ((x[splits[i]:splits[i+1]], y[splits[i]:splits[i+1]]) for i in range(len(splits) - 1))

####### Modules
import torch
import torch.nn as nn
import math

def FilterFunction(h,S,x):
  K = h.shape[0] #Number of filter taps
  B = x.shape[0] #batch size
  N = x.shape[1] #number of nodes

  x = x.reshape([B,1,N])
  S = S.reshape([1,N,N])
  z = x
  for k in range(1,K):
    x = torch.matmul(x,S)
    xS = x.reshape([B,1,N])
    z = torch.cat((z,xS), dim =1)
  y = torch.matmul(z.permute(0,2,1).reshape([B,N,K]),h)
  return y
class GraphFilter(nn.Module):
  def __init__(self,gso,k):
    super().__init__()
    self.gso = torch.tensor(gso)
    self.n = gso.shape[0]
    self.k = k
    self.weight = nn.Parameter(torch.randn(self.k))
    self.reset_parameters()
  def reset_parameters(self):
    stdv = 1./math.sqrt(self.k)
    # Use nn.init.uniform_ for in-place uniform initialization
    nn.init.uniform_(self.weight.data, -stdv, stdv)
  def forward(self,x):
    return FilterFunction(self.weight, self.gso, x)

import numpy as np
import torch
torch.set_default_dtype(torch.float64)
import torch.nn as nn
import torch.optim as optim
import copy
import matplotlib.pyplot as plt

########### Data Generation ###########
N = 50
S = sbm(n=N)
S = normalize_gso(S)
nTrain = 2000
nTest  = 100

z = generate_diffusion(gso = S, n_samples=(nTrain + nTest), n_sources=10)
x,y = data_from_diffusion(z)

trainData,testData = split_data(x,y,(nTrain,nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]

xTrain = torch.tensor(xTrain)
yTrain = torch.tensor(yTrain)
xTest  = torch.tensor(xTest)
yTest  = torch.tensor(yTest)
loss = nn.MSELoss()
########## Architectures ###########
graphFilter = GraphFilter(S,8)
######### Training #########
validationInterval = 5
nEpochs = 30
batchSize = 200
learningRate = 0.05

nValid = int(np.floor(0.01*nTrain))
xValid = xTrain[0:nValid,:]
yValid = yTrain[0:nValid,:]
xTrain = xTrain[nValid:,:]
yTrain = yTrain[nValid:,:]
nTrain = xTrain.shape[0]

optimizer = optim.Adam(graphFilter.parameters(), lr = learningRate)

if nTrain < batchSize:
  nBatches = 1
  batchSize = [nTrain]
elif nTrain % batchSize != 0 :
  nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches
  while sum(batchSize) != nTrain:
    batchSize[-1] -= 1
else:
  nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches
batchIndex = np.cumsum(batchSize).tolist()
batchIndex = [0] + batchIndex

epoch = 0 #epoch counter
##Storing the training

lossTrain = dict()
lossValid = dict()
lossTestBest = dict()
lossTestLast = dict()
bestModel = dict()
lossTrain = []
lossValid = []
while epoch < nEpochs:
  randomPermutation = np.random.permutation(nTrain)
  idxEpoch = [int(i) for i in randomPermutation]
  print("")
  print("Epoch %d" % (epoch+1))
  batch = 0
  while batch < nBatches:
    thisBatchIndices = idxEpoch[batchIndex[batch] : batchIndex[batch+1]]
    xTrainBatch = xTrain[thisBatchIndices,:]
    yTrainBatch = yTrain[thisBatchIndices,:]
    if (epoch * nBatches + batch) % validationInterval == 0:
        print("")
        print("    (E: %2d, B: %3d)" % (epoch+1, batch+1),end = ' ')
        print("")
    graphFilter.zero_grad()
    yHatTrainBatch = graphFilter(xTrainBatch)
    lossValueTrain = loss(yHatTrainBatch, yTrainBatch)
    lossValueTrain.backward()
    optimizer.step()
    lossTrain += [lossValueTrain.item()]
    if (epoch * nBatches + batch) % validationInterval == 0:
      with torch.no_grad():
        yHatValid = graphFilter(xValid)
        lossValueValid = loss(yHatValid, yValid)
        lossValid += [lossValueValid.item()]
        lossValueValid = loss(yHatValid, yValid)

        lossValid += [lossValueValid.item()]

        print("\t Graph Filter: %6.4f [T]" % (
                    lossValueTrain) + " %6.4f [V]" % (
                    lossValueValid))

            # Saving the best model so far
        if len(lossValid) > 1:
          if lossValueValid <= min(lossValid):
            bestModel =  copy.deepcopy(graphFilter)
          else:
            bestModel =  copy.deepcopy(graphFilter)

    batch+=1

  epoch+=1

print("")

plt.plot(lossTrain)
plt.ylabel('Training loss')
plt.xlabel('Step')
plt.show()

print("Final evaluation results")

with torch.no_grad():
    yHatTest = graphFilter(xTest)
lossTestLast = loss(yHatTest, yTest)
lossTestLast = lossTestLast.item()
with torch.no_grad():
    yHatTest = bestModel(xTest)
lossTestBest = loss(yHatTest, yTest)
lossTestBest = lossTestBest.item()

print(" Graph Filter: %6.4f [Best]" % (
                    lossTestBest) + " %6.4f [Last]" % (
                    lossTestLast))

"""#**Perceptron Design**"""

#### Data generation ####
import numpy as np
#### stochastic block matrix formation
def sbm(n=50 ,c=5, p_intra = 0.6,p_inter = 0.2):
  community = np.repeat(list(range(c)), np.ceil(n/c))
  community = community[0:n]
  community = np.expand_dims(community, axis = 1)
  intra = community == community.T
  inter = np.logical_not(intra)
  random = np.random.random((n,n))
  tri = np.tri(n , k=1)
  graph = np.zeros((n,n))
  graph[np.logical_and.reduce([tri, intra, random < p_intra])] = 1
  graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1
  graph += graph.T
  return graph
def normalize_gso(gso):
  eig_val, eig_vec = np.linalg.eig(gso)
  return gso/np.max(np.abs(eig_val))
def generate_diffusion(gso,n_samples = 2100, n_sources = 10):
  n = gso.shape[0] ##number of nodes
  z = np.zeros((n_samples,n,11,1))
  for i in range(n_samples):
    sources = np.random.choice(n,n_sources, replace = False)
    z[i,sources,0,0] = np.random.uniform(0,10)
  mu = np.zeros(n)
  sigma = np.eye(n) * 1e-3
  for t in range(10):
    ### noise generation
    noise = np.random.multivariate_normal(mu,sigma,n_samples)
    z[:,:,t+1] = gso @ z[:,:,t] + np.expand_dims(noise,-1) ## adding an extra column
  z = z.transpose((0,2,1,3))
  return z.squeeze()
def data_from_diffusion(z):
  z = np.random.permutation(z)
  y = np.expand_dims(z[:,0,:], 1)
  x = np.zeros(y.shape)
  for i,sample in enumerate(z):
    x[i] = sample[4]
  return x.squeeze(), y.squeeze()
def split_data(x,y, splits = (2000,100)):
  splits = np.cumsum([0] + list(splits))
  splits = (splits*x.shape[0]/splits[-1]).astype(int)
  return ((x[splits[i]:splits[i+1]], y[splits[i]:splits[i+1]]) for i in range(len(splits) -1 ))

#### Modules designing ####
import torch
import torch.nn as nn
import math

def FilterFunction(h, S, x):
  K = h.shape[0] ## number of filter taps
  B = x.shape[0]
  N = x.shape[1]

  x = x.reshape([B, 1, N])
  S = S.reshape([1, N, N])
  z = x
  for k in range(1,K):
    x = torch.matmul(x,S)
    xS = x.reshape([B, 1, N])
    z = torch.cat((z,xS), dim=1)
  y = torch.matmul(z.permute(0,2,1).reshape([B,N,K]),h)
  return y
class GraphFilter(nn.Module):
  def __init__(self,gso,k):
    super().__init__()
    self.gso = gso
    self.k = k
    self.n = gso.shape[0]
    self.weight = nn.Parameter(torch.randn(self.k))
    self.reset_parameters()
  def reset_parameters(self):
    stdv = 1./ math.sqrt(self.k)
    self.weight.data.uniform_(-stdv,stdv)
  def forward(self,x):
    return FilterFunction(self.weight,self.gso,x)
class GraphPerceptron(nn.Module):
  def __init__(self,gso,k,sigma):
    self.gso = gso
    self.k = k
    self.n = gso.shape[0]
    self.sigma = sigma
    self.weight = nn.Parameter(torch.randn(self.k))
    self.reset_parameters()
  def reset_parameters(self):
    stdv = 1./ math.sqrt(self.k)
    self.weight.data.uniform_(-stdv,stdv)
  def forward(self,x):
    y = FilterFunction(self.weight , self.gso, x)
    y = self.sigma(y)
    return y

#### Data generation ####
import numpy as np
#### stochastic block matrix formation
def sbm(n=50, c=5, p_intra=0.6, p_inter=0.2):
    community = np.repeat(list(range(c)), np.ceil(n/c))
    community = community[0:n]
    community = np.expand_dims(community, axis=1)
    intra = community == community.T
    inter = np.logical_not(intra)
    random = np.random.random((n,n))
    tri = np.tri(n, k=-1)  # Changed k=1 to k=-1 to get lower triangle including diagonal
    graph = np.zeros((n,n))  # Added parentheses to create proper numpy array
    graph[np.logical_and.reduce([tri, intra, random < p_intra])] = 1
    graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1
    graph += graph.T
    return graph

def normalize_gso(gso):
    eig_val, eig_vec = np.linalg.eig(gso)
    return gso/np.max(np.abs(eig_val))

def generate_diffusion(gso, n_samples=2100, n_sources=10):
    n = gso.shape[0]  # number of nodes
    z = np.zeros((n_samples, n, 11, 1))  # Fixed dimensions
    for i in range(n_samples):
        sources = np.random.choice(n, n_sources, replace=False)
        z[i, sources, 0, 0] = np.random.uniform(0, 10)
    mu = np.zeros(n)
    sigma = np.eye(n) * 1e-3
    for t in range(10):
        ### noise generation
        noise = np.random.multivariate_normal(mu, sigma, n_samples)
        z[:, :, t+1] = gso @ z[:, :, t] + np.expand_dims(noise, -1)  # adding an extra column
    z = z.transpose((0, 2, 1, 3))  # Moved outside the loop
    return z.squeeze()

def data_from_diffusion(z):
    z = np.random.permutation(z)
    y = np.expand_dims(z[:, 0, :], 1)
    x = np.zeros(y.shape)
    for i, sample in enumerate(z):
        x[i] = sample[4]
    return x.squeeze(), y.squeeze()

def split_data(x, y, splits=(2000, 100)):
    splits = np.cumsum([0] + list(splits))
    splits = (splits * x.shape[0] / splits[-1]).astype(int)  # Convert to integers
    return ((x[splits[i]:splits[i+1]], y[splits[i]:splits[i+1]]) for i in range(len(splits) - 1))

#### Modules designing ####
import torch
import torch.nn as nn
import math

def FilterFunction(h, S, x):
    K = h.shape[0]  # number of filter taps
    B = x.shape[0]
    N = x.shape[1]

    x = x.reshape([B, 1, N])
    S = S.reshape([1, N, N])
    z = x
    for k in range(1, K):
        x = torch.matmul(x, S)
        xS = x.reshape([B, 1, N])
        z = torch.cat((z, xS), dim=1)
    y = torch.matmul(z.permute(0, 2, 1).reshape([B, N, K]), h)
    return y

class GraphFilter(nn.Module):
    def __init__(self, gso, k):
        super().__init__()
        self.gso = gso
        self.k = k
        self.n = gso.shape[0]
        self.weight = nn.Parameter(torch.randn(self.k))
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.k)
        self.weight.data.uniform_(-stdv, stdv)

    def forward(self, x):
        return FilterFunction(self.weight, self.gso, x)

class GraphPerceptron(nn.Module):
    def __init__(self, gso, k, sigma):
        super().__init__()  # Added missing super().__init__()
        self.gso = gso
        self.k = k
        self.n = gso.shape[0]
        self.sigma = sigma
        self.weight = nn.Parameter(torch.randn(self.k))
        self.reset_parameters()

    def reset_parameters(self):
        stdv = 1. / math.sqrt(self.k)
        self.weight.data.uniform_(-stdv, stdv)

    def forward(self, x):
        y = FilterFunction(self.weight, self.gso, x)
        y = self.sigma(y)
        return y

########## Main Execution ##########
import torch
torch.set_default_dtype(torch.float64)
import torch.nn as nn
import torch.optim as optim
import copy

# Data Generation
N = 50
S = sbm(n=N)
S = normalize_gso(S)
S = torch.tensor(S)  # Convert to tensor for PyTorch operations

nTrain = 2000
nTest = 100
nEpochs = 10  # Added missing nEpochs definition
validationInterval = 30  # Added missing validationInterval definition

z = generate_diffusion(gso=S.numpy(), n_samples=nTrain+nTest)  # Convert S to numpy for this function
x, y = data_from_diffusion(z)

trainData, testData = split_data(x, y, (nTrain, nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]

xTrain = torch.tensor(xTrain)
yTrain = torch.tensor(yTrain)
xTest = torch.tensor(xTest)
yTest = torch.tensor(yTest)

loss = nn.MSELoss()

# Graph Perceptron
graphPerceptron = GraphPerceptron(S, 8, nn.ReLU())

# Training
validation_interval = 30
batchSize = 200
learningRate = 0.05

nValid = int(np.floor(0.01 * nTrain))
xValid = xTrain[0:nValid, :]
yValid = yTrain[0:nValid, :]
xTrain = xTrain[nValid:, :]
yTrain = yTrain[nValid:, :]
nTrain = xTrain.shape[0]  # Update nTrain after validation split

optimizer = optim.Adam(graphPerceptron.parameters(), lr=learningRate)
if nTrain < batchSize:
    nBatches = 1
    batchSize = [nTrain]
elif nTrain % batchSize != 0:
    nBatches = np.ceil(nTrain / batchSize).astype(np.int64)
    batchSize = [batchSize] * nBatches
    while sum(batchSize) != nTrain:
        batchSize[-1] -= 1
else:
    nBatches = np.int64(nTrain / batchSize)
    batchSize = [batchSize] * nBatches
batchIndex = np.cumsum(batchSize).tolist()
batchIndex = [0] + batchIndex

epoch = 0
lossTrain = []
lossValid = []

bestModel = None
bestLoss = float('inf')

while epoch < nEpochs:
    randomPermutation = np.random.permutation(nTrain)
    idxEpoch = [int(i) for i in randomPermutation]
    print("")
    print("Epoch %d" % (epoch + 1))
    batch = 0
    while batch < nBatches:
        thisBatchIndices = idxEpoch[batchIndex[batch]:batchIndex[batch + 1]]
        xTrainBatch = xTrain[thisBatchIndices, :]
        yTrainBatch = yTrain[thisBatchIndices, :]

        if (epoch * nBatches + batch) % validationInterval == 0:
            print("")
            print("    (E: %2d, B: %3d)" % (epoch + 1, batch + 1), end=' ')
            print("")

        graphPerceptron.zero_grad()
        yHatTrainBatch = graphPerceptron(xTrainBatch)
        lossValueTrain = loss(yHatTrainBatch, yTrainBatch)
        lossValueTrain.backward()
        optimizer.step()
        lossTrain.append(lossValueTrain.item())

        if (epoch * nBatches + batch) % validationInterval == 0:
            with torch.no_grad():
                yHatValid = graphPerceptron(xValid)
                lossValueValid = loss(yHatValid, yValid)
                lossValid.append(lossValueValid.item())

                print("\t Graph Perceptron: %6.4f [T]" % lossValueTrain + " %6.4f [V]" % lossValueValid)

                # Saving the best model so far
                if lossValueValid < bestLoss:
                    bestLoss = lossValueValid
                    bestModel = copy.deepcopy(graphPerceptron.state_dict())

        batch += 1
    epoch += 1

# Load best model for testing
if bestModel is not None:
    graphPerceptron.load_state_dict(bestModel)

# Test the model
with torch.no_grad():
    yHatTest = graphPerceptron(xTest)
    testLoss = loss(yHatTest, yTest)
    print("\nTest Loss: %6.4f" % testLoss)
plt.plot(lossTrain)

plt.plot(lossValid)

import numpy as np
import torch
torch.set_default_dtype(torch.float64)
import torch.nn as nn
import matplotlib.pyplot as plt
import torch.optim as optim
import copy

N = 50
learningRate = 0.05
S = sbm(n=N)
S = normalize_gso(S)
S = torch.tensor(S)
nTrain = 2000
nTest  = 100
z = generate_diffusion(gso = S, n_samples = (nTrain + nTest))
x,y = data_from_diffusion(z)
trainData,testData = split_data(x,y, (nTrain,nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]

xTrain = torch.tensor(xTrain)
yTrain = torch.tensor(yTrain)
xTest = torch.tensor(xTest)
yTest = torch.tensor(yTest)

loss = nn.MSELoss()
graphPerceptron = GraphPerceptron(S, 8, nn.ReLU())
validationInterval = 5
nEpochs = 30
batchSize = 200
learningRate = 0.05

nValid = int(float(0.01*nTrain)) # Fixed np.float to float
xValid = xTrain[0:nValid,:] # Fixed variable assignment
yValid = yTrain[0:nValid,:] # Fixed variable assignment
xTrain = xTrain[nValid:,:] # Fixed variable assignment
yTrain = yTrain[nValid:,:] # Fixed variable assignment
nTrain = xTrain.shape[0]

optimizer = optim.Adam(graphPerceptron.parameters(), lr=learningRate)
if nTrain < batchSize:
  nBatches = 1
  batchSize = [nTrain]
elif nTrain % batchSize != 0:
  nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize]*nBatches # Fixed typo batcchSize to batchSize
  while sum(batchSize) != nTrain:
    batchSize[-1] -= 1
else:
  nBatches = np.int64(nTrain/batchSize)
  batchSize = [batchSize]*nBatches
batchIndex = np.cumsum(batchSize).tolist()
batchIndex = [0] + batchIndex

epoch = 0 ## epoch counter
lossTrain = dict()
lossValid = dict()
lossTestBest = dict()
lossTestLast = dict()

bestModel = dict()
lossTrain = []
lossValid = []

while epoch < nEpochs:
  randomPermutation = np.random.permutation(nTrain)
  idxEpoch = [int(i) for i in randomPermutation]
  batch = 0
  while batch < nBatches:
    thisBatchIndices = idxEpoch[batchIndex[batch]:batchIndex[batch+1]]
    xTrainBatch = xTrain[thisBatchIndices, :]
    yTrainBatch = yTrain[thisBatchIndices, :]
    graphPerceptron.zero_grad()
    yHatTrainBatch = graphPerceptron(xTrainBatch)
    lossValueTrain = loss(yHatTrainBatch, yTrainBatch)
    lossValueTrain.backward()
    optimizer.step()
    lossTrain.append(lossValueTrain.item())
    if (epoch * nBatches + batch) % validationInterval == 0:
      with torch.no_grad():
        lossValueValid = loss(graphPerceptron(xValid), yValid)
        lossValid += [lossValueValid.item()]
        if len(lossValid) > 1:
          if lossValueValid <= min(lossValid):
            bestModel = copy.deepcopy(graphPerceptron)
          else:
            bestModel = copy.deepcopy(bestModel)
    batch +=1
  epoch += 1

with torch.no_grad():
    yHatTest = graphPerceptron(xTest)
lossTestLast = loss(yHatTest, yTest)
lossTestLast = lossTestLast.item()
with torch.no_grad():
    yHatTest = bestModel(xTest)
lossTestBest = loss(yHatTest, yTest)
lossTestBest = lossTestBest.item()

plt.plot(lossTrain)

plt.plot(lossValid)

#### Data Generation
import numpy as np
import torch # Import torch

def sbm(n = 50, c=5 ,p_intra = 0.6, p_inter=0.2):
  community=np.repeat(list(range(c)), np.ceil(n/c))
  community = community[0:n]
  community = np.expand_dims(community, axis=1)
  intra = community == community.T
  inter = np.logical_not(intra)
  ### generating random matrix with entries between 0 and 1
  random = np.random.random((n,n)) ### Entry Point
  tri = np.tri(n,k=-1) # upper half
  graph = np.zeros((n,n))
  graph[np.logical_and.reduce([tri, intra, random < p_intra])] = 1
  graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1
  graph += graph.T
  return graph
def normalize_gso(gso):
  eig_val,eig_vec = np.linalg.eig(gso)
  return gso/np.max(eig_val.real)


def generate_diffusion(gso, n_samples = 2100 ,n_sources = 10):
  n  = gso.shape[0] #number of nodes
  # Convert gso to torch tensor if it's not already
  if not isinstance(gso, torch.Tensor):
      gso = torch.tensor(gso, dtype=torch.float64) # Ensure dtype matches default
  z = torch.zeros((n_samples, n, 11, 1), dtype=torch.float64) # Use torch.zeros
  for i in range(n_samples):
    sources = np.random.choice(n,n_sources,replace=False)
    z[i,sources,0,0] = torch.rand(n_sources) * 10 # Use torch.rand
  mu = torch.zeros(n, dtype=torch.float64) # Use torch.zeros
  sigma = torch.eye(n, dtype=torch.float64) * 1e-3 # Use torch.eye
  for t in range(10):
    noise = torch.distributions.multivariate_normal.MultivariateNormal(mu, sigma).sample((n_samples,)) # Use torch distributions for noise
    z[:,:,t+1] = torch.matmul(gso, z[:,:,t]) + noise.unsqueeze(-1) # Use torch.matmul and unsqueeze
  z = z.transpose(1,2) # Use torch.transpose
  return z.squeeze()

def data_from_diffusion(z):
  # Assuming z is a torch tensor, use torch operations
  z = z[torch.randperm(z.shape[0])] # Use torch.randperm for permutation
  y = z[:,0,:].unsqueeze(1) # Use torch.unsqueeze
  x = torch.zeros_like(y) # Use torch.zeros_like
  x = z[:,4,:] # Corrected indexing for x (z_4)
  return x.squeeze(),y.squeeze() # Use squeeze for consistency

def split_data(x,y, splits=(2000,100)):
  #defining the index
  splits = np.cumsum([0] + list(splits))
  splits = (splits * x.shape[0]/splits[-1]).astype(int) #scaling to avoid blowing up
  #returning the training and testing data as tuples
  return((x[splits[i]:splits[i+1]], y[splits[i]:splits[i+1]]) for i in range(len(splits)-1))

### Defining the modules
import torch
import torch.nn as nn
import math

def FilterFunction(h, S, x):
  F = h.shape[0] #number of output features
  K = h.shape[1] #number of filter taps
  G = h.shape[2] #number of input features
  N = S.shape[0] #number of nodes
  B = x.shape[0] #batch size

  # Ensure x has shape [B, G, N]
  x = x.reshape([B, G, N])
  # Ensure S has shape [N, N]
  S = S.reshape([N, N])
  z = x.unsqueeze(2) # Add a dimension for filter taps: [B, G, 1, N]

  # Loop over number of filter taps
  for k in range(1, K):
    # Apply S to the last dimension (nodes)
    x = torch.matmul(x, S) # [B, G, N] @ [N, N] -> [B, G, N]
    z = torch.cat((z, x.unsqueeze(2)), dim=2) # Concatenate along the filter tap dimension: [B, G, k+1, N]

  # Reshape z to [B, N, K*G]
  z = z.permute(0, 3, 2, 1).reshape([B, N, K * G])

  # Reshape h to [K*G, F]
  h = h.reshape([F, K * G]).permute(1, 0)

  # Matrix multiplication: [B, N, K*G] @ [K*G, F] -> [B, N, F]
  y = torch.matmul(z, h)

  # Permute to [B, F, N]
  y = y.permute(0, 2, 1)
  return y

class GraphFilter(nn.Module):
  def __init__(self,gso,k,f_in,f_out):
    super().__init__()
    self.gso = torch.tensor(gso)
    self.n   = gso.shape[0]
    self.k   = k
    self.f_in = f_in
    self.f_out = f_out
    self.weight = nn.Parameter(torch.randn(self.f_out,self.k,self.f_in))
    self.reset_parameters()
  def reset_parameters(self):
    stdv = 1./math.sqrt(self.f_in*self.k)
    nn.init.uniform_(self.weight.data, -stdv, stdv) # Use nn.init.uniform_
  def forward(self,x):
    return FilterFunction(self.weight, self.gso, x)
class GNN(nn.Module):
  def __init__(self,gso,l,k,f,sigma):
    super().__init__()
    self.gso = torch.tensor(gso)
    self.n = gso.shape[0]
    self.l = l
    self.k = k
    self.f = f
    self.sigma = sigma
    gml = []
    for layer in range(l):
      gml.append(GraphFilter(gso,k[layer],f[layer],f[layer+1]))
      if layer < l -1: # Add activation after hidden layers
        gml.append(sigma)
    self.gml = nn.Sequential(*gml)
  def forward(self,x):
    return self.gml(x)

import numpy as np
import torch
torch.set_default_dtype(torch.float64)
import torch.nn as nn
import torch.optim as optim
import copy
import matplotlib.pyplot as plt

N = 50
S = sbm(n=N)
S = normalize_gso(S)
nTrain = 2000
nTest = 100
z = generate_diffusion(gso=S, n_samples=nTrain+nTest)

x,y = data_from_diffusion(z)

trainData,testData = split_data(x,y, (nTrain,nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest  = testData[0]
yTest  = testData[1]

# Reshape xTrain, yTrain, xTest, yTest to [Batch_size, Features, Nodes]
xTrain = torch.tensor(xTrain).reshape([-1, 1, N]) # Input features G=1
yTrain = torch.tensor(yTrain).reshape([-1, 1, N]) # Output features F=1
xTest = torch.tensor(xTest).reshape([-1, 1, N])   # Input features G=1
yTest = torch.tensor(yTest).reshape([-1, 1, N])   # Output features F=1

loss = nn.MSELoss()
architectures = dict()

MLgraphFilter = nn.Sequential(GraphFilter(S,8,1,32),nn.ReLU(),GraphFilter(S,8,32,1))
architectures['MFGraphFilter'] = MLgraphFilter
GNN1Ly = GNN(S,2,[8,1],[1,32,1],nn.ReLU())
architectures['GNN 1 layer'] = GNN1Ly

GNN2Ly = GNN(S,3,[5,5,1],[1,16,4,1],nn.ReLU())
architectures['GNN 2 layer'] = GNN2Ly

####### Training ########

validationInterval = 5

nEpochs = 30
batchSize = 200
learningRate = 0.05

nValid = int(np.floor(0.01*nTrain))
xValid = xTrain[0:nValid,:,:]
yValid = yTrain[0:nValid,:,:]
xTrain = xTrain[nValid:,:,:]
yTrain = yTrain[nValid:,:,:]
nTrain = xTrain.shape[0]

optimizers = dict()
for key in architectures.keys():
    optimizers[key] = optim.Adam(architectures[key].parameters(), lr=learningRate)

if nTrain < batchSize:
    nBatches = 1
    batchSize = [nTrain]
elif nTrain % batchSize != 0:
    nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
    batchSize = [batchSize]*nBatches
    while sum(batchSize) != nTrain:
        batchSize[-1] -=1
else:
    nBatches = np.int64(nTrain/batchSize)
    batchSize = [batchSize]*nBatches
batchIndex = np.cumsum(batchSize).tolist()
batchIndex = [0] + batchIndex

epoch = 0

lossTrain = dict()
costTrain = dict()
lossValid = dict()
costValid = dict()

lossTestBest = dict()
lossTestLast = dict()
costTestBest = dict()
costTestLast = dict()

bestModel = dict()
for key in architectures.keys():
    lossTrain[key] = []
    costTrain[key] = []
    lossValid[key] = []
    costValid[key] = []
    bestModel[key] = None # Initialize bestModel for each architecture

while epoch < nEpochs:
    randomPermutation = np.random.permutation(nTrain)
    idxEpoch = [int(i) for i in randomPermutation]
    batch = 0
    while batch < nBatches:
        thisBatchIndices = idxEpoch[batchIndex[batch]:batchIndex[batch+1]]
        xTrainBatch = xTrain[thisBatchIndices,:,:]
        yTrainBatch = yTrain[thisBatchIndices,:,:]
        for key in architectures.keys():
            architectures[key].zero_grad()
            yHatTrainBatch = architectures[key](xTrainBatch)
            lossValueTrain = loss(yHatTrainBatch,yTrainBatch) # Removed squeeze()
            lossValueTrain.backward()
            optimizers[key].step()
            costValueTrain = lossValueTrain.item()
            lossTrain[key] += [lossValueTrain.item()]
            costTrain[key] += [costValueTrain]
            if (epoch * nBatches + batch) % validationInterval == 0:
                with torch.no_grad():
                    yHatValid = architectures[key](xValid)
                lossValueValid = loss(yHatValid,yValid) # Removed squeeze()
                costValueValid = lossValueValid.item()
                lossValid[key] += [lossValueValid.item()]
                costValid[key] += [costValueValid]
                if bestModel[key] is None or costValueValid <= min(costValid[key]): # Corrected condition for saving best model
                     bestModel[key] = copy.deepcopy(architectures[key])

        batch += 1
    epoch += 1

for key in architectures.keys():
  plt.plot(lossTrain[key])
  plt.plot(lossValid[key])
  # plt.plot(costTrain[key])
  # plt.plot(costValid[key])
  plt.title(key)
  plt.show()

#### Evaluation
for key in architectures.keys():
  with torch.no_grad():
    yHatTest = architectures[key](xTest)
  lossTestLast[key] = loss(yHatTest.squeeze(), yTest.squeeze())
  costTestLast[key] = lossTestLast[key].item()
  with torch.no_grad():
    yHatTest = bestModel[key](xTest)
  lossTestBest[key] = loss(yHatTest.squeeze(), yTest.squeeze())
  costTestBest[key] = lossTestBest[key].item()

"""#**4 layer GNN trial**"""

import numpy as np
import torch
import torch.nn as nn
torch.set_default_dtype(torch.float64)
import copy
import matplotlib.pyplot as plt

### Generating the data
N = 50
S = sbm(n=N)
S = normalize_gso(S)
nTrain = 2000
nTest  = 100

z = generate_diffusion(gso=S, n_samples=nTrain+nTest)
x,y = data_from_diffusion(z)

trainData,testData = split_data(x,y,(nTrain,nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest  = testData[0]
yTest  = testData[1]

xTrain = torch.tensor(xTrain)
xTrain = xTrain.reshape([-1,1,N])
yTrain = torch.Tensor(yTrain)
yTrain = yTrain.reshape([-1,1,N]) # Reshape yTrain to match model output shape

xTest = torch.tensor(xTest)
xTest = xTest.reshape([-1,1,N])
yTest = torch.Tensor(yTest)
yTest = yTest.reshape([-1,1,N]) # Reshape yTest to match model output shape

### Loss Function calculator
loss = nn.MSELoss()

####Architectures development for Data Storage
architectures = dict()
MLgraphFilter = nn.Sequential(GraphFilter(S,8,1,32),nn.ReLU(),GraphFilter(S,8,32,1))
architectures['MLGraphFilter'] = MLgraphFilter

### GNN Layer -> 4
GNN4Ly = GNN(S,4,[8,10,9,9,1],[1,32,31,35,1],nn.ReLU()) # Corrected GNN initialization and layer parameters
architectures['GNN 4 layer'] = GNN4Ly

### Training ###
validationInterval = 5

nEpochs = 30
batchSize = 200
learningRate = 0.05

nValid = int(np.floor(0.01*nTrain))
xValid = xTrain[0:nValid,:,:]
yValid = yTrain[0:nValid,:,:]
xTrain = xTrain[nValid:,:,:]
yTrain = yTrain[nValid:,:,:]
nTrain = xTrain.shape[0]

optimizers = dict()
for key in architectures.keys():
  optimizers[key] = optim.Adam(architectures[key].parameters(),lr=learningRate)
if nTrain < batchSize:
  nBatches = 1
  batchSize = [nTrain]
elif nTrain % batchSize != 0:
  nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches
  while sum(batchSize) != nTrain:
    batchSize[-1] -= 1
else:
  nBatches = np.int(nTrain/batchSize)
  batchSize = [batchSize] * nBatches
batchIndex = np.cumsum(batchSize).tolist()
batchIndex = [0] + batchIndex
epoch = 0

### Storing the training
lossTrain = dict()
lossValid = dict()
costTrain = dict()
costValid = dict()

### Storing the test variables
lossTestBest = dict()
costTestBest = dict()
lossTestLast = dict()
costTestLast = dict()

bestModel = dict()
for key in architectures.keys():
  lossTrain[key] = []
  costTrain[key] = []
  lossValid[key] = []
  costValid[key] = []
  bestModel[key] = None
while epoch < nEpochs:
  randomPermutation = np.random.permutation(nTrain)
  idxEpoch = [int(i) for i in randomPermutation]
  batch = 0
  while batch < nBatches:
    thisBatchIndices = idxEpoch[batchIndex[batch]:batchIndex[batch + 1]]
    xTrainBatch = xTrain[thisBatchIndices,:,:]
    yTrainBatch = yTrain[thisBatchIndices,:,:]
    for key in architectures.keys():
      architectures[key].zero_grad()
      yHatTrainBatch = architectures[key](xTrainBatch)
      lossValueTrain = loss(yHatTrainBatch.squeeze(),yTrainBatch.squeeze())
      lossValueTrain.backward()
      optimizers[key].step()
      costValueTrain = lossValueTrain.item()
      lossTrain[key] +=[lossValueTrain.item()]
      costTrain[key] += [costValueTrain]
      if (epoch * nBatches + batch) % validationInterval == 0:
        with torch.no_grad(): ### GNN Output
          yHatValid = architectures[key](xValid)
        lossValueValid = loss(yHatValid.squeeze(), yValid.squeeze())
        costValueValid = lossValueValid.item()
        lossValid[key] += [lossValueValid.item()]
        costValid[key] += [costValueValid]
        if len(costValid[key]) > 1:
          if costValueValid <= min(costValid[key]):
            bestModel[key] = copy.deepcopy(architectures[key])
          else:
            bestModel[key] = copy.deepcopy(architectures[key])
    batch +=1
  epoch+=1

### Final Evaluation ###
for key in architectures.keys():
  with torch.no_grad():
    yHatTest = architectures[key](xTest)
  lossTestLast[key] = loss(yHatTest.squeeze(),yTest.squeeze())
  costTestLast[key] = lossTestLast[key].item()
  with torch.no_grad():
    yHatTest = bestModel[key](xTest)
  lossTestBest[key] = loss(yHatTest.squeeze(),yTest.squeeze())
  costTestBest[key] = lossTestBest[key].item()

fig,ax = plt.subplots(figsize=(10,6))
for key in architectures.keys():
  ax.plot(lossTrain[key],linestyle='--', label=f'{key} Train')
  ax.plot(lossValid[key],marker='^', label=f'{key} Valid')
ax.set_title("Training and Validation Loss",fontweight='bold')
plt.legend()
plt.grid()
plt.show()

"""#**Transferability and Graph Change $S_1^{N_1 X N_1} → S_2^{N_2 X N_2}$**"""

#### Data generation
import numpy as np

def sbm(n=50, c=5, p_intra=0.6, p_inter=0.2):
  community = np.repeat(list(range(c)), np.ceil(n/c))
  community = community[0:n]
  community = np.expand_dims(community,1)
  intra = community == community.T
  inter = np.logical_not(intra)
  random = np.random.random((n,n))
  tri = np.tri(n,k=-1)
  graph = np.zeros((n,n))
  graph[np.logical_and.reduce([tri, intra, random < p_intra])] = 1
  graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1
  graph += graph.T
  return graph
def normalize_gso(gso):
  eig_val,eig_vec = np.linalg.eig(gso)
  return gso/np.max(eig_val.real)

def generate_diffusion(gso, n_samples=2100, n_sources=10):
  n = gso.shape[0]
  z = np.zeros((n_samples,n,11,1)) # Corrected dimensions to match other generate_diffusion
  for i in range(n_samples):
    sources = np.random.choice(n,n_sources,replace=False)
    z[i,sources,0,0] = np.random.uniform(0,10)
  mu =np.zeros(n)
  sigma = np.eye(n)*1e-3
  for t in range(10): # Corrected range to match other generate_diffusion
    noise = np.random.multivariate_normal(mu,sigma,n_samples)
    z[:,:,t+1] = gso @ z[:,:,t] + np.expand_dims(noise,-1)
  z = z.transpose((0,2,1,3)) # Moved transpose outside the loop
  return z.squeeze()


def data_from_diffusion(z):
  z = np.random.permutation(z)
  y = np.expand_dims(z[:,0,:],1)
  x = np.zeros(y.shape)
  for i,sample in enumerate(z):
    x[i] = sample[4] # Corrected indexing from z[4] to sample[4]
  return x,y

def split_data(x,y, splits=(2000,100)):
  splits = np.cumsum([0] + list(splits))
  splits = (splits * x.shape[0] / splits[-1]).astype(int)
  return ((x[splits[i]:splits[i+1]],y[splits[i]:splits[i+1]]) for i in range(len(splits) -1 ))

### Module defination

import torch
import torch.nn as nn
import math

def FilterFunction(h, S, x, b=None):
  # Ensure h has shape [F, K, G]
  F = h.shape[0]
  K = h.shape[1]
  G = h.shape[2]

  N = S.shape[0]
  B = x.shape[0]

  # Ensure x has shape [B, G, N]
  x = x.reshape([B, G, N])
  # Ensure S has shape [N, N]
  S = S.reshape([N, N])

  z = x.unsqueeze(2) # Add a dimension for filter taps: [B, G, 1, N]

  # Loop over number of filter taps
  for k in range(1, K):
    # Apply S to the last dimension (nodes)
    x = torch.matmul(x, S) # [B, G, N] @ [N, N] -> [B, G, N]
    z = torch.cat((z, x.unsqueeze(2)), dim=2) # Concatenate along the filter tap dimension: [B, G, k+1, N]

  # Reshape z to [B, N, K*G]
  z = z.permute(0, 3, 2, 1).reshape([B, N, K * G])

  # Reshape h to [K*G, F]
  h = h.reshape([F, K * G]).permute(1, 0)

  # Matrix multiplication: [B, N, K*G] @ [K*G, F] -> [B, N, F]
  y = torch.matmul(z, h)

  if b is not None:
    # Ensure bias has shape [F] or [1, F, 1] for broadcasting
    # Assuming bias is [F, 1] and we want to add it to the last dimension of y [B, N, F]
    y = y + b.squeeze() # Squeeze bias to [F] for broadcasting

  # Permute to [B, F, N]
  y = y.permute(0, 2, 1)
  return y

class GraphFilter(nn.Module):
  def __init__(self,gso,k,f_in,f_out,bias): # Added k as parameter
    super().__init__()
    self.gso = torch.tensor(gso)
    self.n = gso.shape[0]
    self.k = k
    self.f_in = f_in
    self.f_out = f_out
    self.bias = None # Initialize bias to None
    if bias:
      self.bias = nn.parameter.Parameter(torch.Tensor(self.f_out, 1))
    self.weight = nn.Parameter(torch.randn(self.f_out,self.k,self.f_in))
    self.reset_parameters()
  def reset_parameters(self):
    stdv = 1. / math.sqrt(self.f_in * self.k)
    self.weight.data.uniform_(-stdv, stdv)
    if self.bias is not None:
      self.bias.data.uniform_(-stdv, stdv)
  def forward(self,x):
    return FilterFunction(self.weight, self.gso, x, self.bias)
  def changeGSO(self,new_gso):
    self.gso = torch.tensor(new_gso)
    self.n = new_gso.shape[0]
class GNN(nn.Module):
  def __init__(self,gso,l,k,f,sigma,bias):
    super().__init__()
    self.gso = torch.tensor(gso) # Convert gso to tensor
    self.n  = gso.shape[0]
    self.l = l
    self.k = k
    self.f = f
    self.sigma = sigma
    self.bias = bias

    gml = []
    for layer in range(l):
      # Pass k[layer] to GraphFilter
      gml.append(GraphFilter(gso,k[layer],f[layer],f[layer+1],bias))
      if layer < l-1: # Add activation after hidden layers
        gml.append(sigma)
    self.gml = nn.Sequential(*gml)
  def forward(self,x):
    return self.gml(x)
  def changeGSO(self,new_gso):
    self.gso = torch.tensor(new_gso) # Convert new_gso to tensor
    for layer in range(len(self.gml)): # Iterate through all modules in sequential
      if isinstance(self.gml[layer], GraphFilter): # Check if the module is a GraphFilter
        self.gml[layer].changeGSO(new_gso)
    self.n = new_gso.shape[0]

import numpy as np
import torch
import torch.nn as nn
import copy
torch.set_default_dtype(torch.float64)
import torch.optim as optim

N = 50
S = sbm(n=N)
S = normalize_gso(S)
nTrain = 2000
nTest = 100

z = generate_diffusion(gso=S,n_samples=nTrain+nTest)
x,y = data_from_diffusion(z)

trainData,testData = split_data(x,y,(nTrain,nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]

xTrain = torch.tensor(xTrain).reshape([-1,1,N])
yTrain = torch.tensor(yTrain).reshape([-1,1,N])
xTest = torch.tensor(xTest).reshape([-1,1,N]) # Reshape xTest
yTest = torch.tensor(yTest).reshape([-1,1,N]) # Reshape yTest

loss = nn.MSELoss()
architectures = dict()
linearParam = torch.nn.Linear(N,N,bias=True)
architectures['LinearLayer'] = linearParam

#### FCNN (Fully Connected Neural Network)
fcNet = nn.Sequential(torch.nn.Linear(N,25,bias=True), nn.ReLU(), torch.nn.Linear(25,N,bias=True),nn.ReLU())
architectures['FCNN'] = fcNet # Added fcNet to architectures
#### Multiple feature graph filter
MLgraphFilter = nn.Sequential(GraphFilter(S,8,1,32,True), GraphFilter(S,1,32,1,True))
architectures['MLGraphFilter'] = MLgraphFilter
### GNN layer count -> 2
GNN1Ly = GNN(S,2,[8,1],[1,32,1],nn.ReLU(),True)
architectures['GNN 1 layer'] = GNN1Ly
### GNN layer count -> 3
GNN2Ly = GNN(S,3,[5,5,1],[1,16,4,1],nn.ReLU(),True)
architectures['GNN 2 layer'] = GNN2Ly

#### Training ####

validationInterval = 5
nEpochs = 30
batchSize = 200
learningRate = 0.05
nValid = int(np.floor(0.01*nTrain))
xValid = xTrain[0:nValid,:,:]
yValid = yTrain[0:nValid,:,:]
xTrain = xTrain[nValid:,:,:]
yTrain = yTrain[nValid:,:,:]
nTrain = xTrain.shape[0]

optimizers = dict()
for key in architectures.keys():
  optimizers[key] = optim.Adam(architectures[key].parameters(),lr=learningRate)

# Corrected batch size calculation
nBatches = (nTrain + batchSize - 1) // batchSize # Calculate number of batches using integer division
batchSizes = [batchSize] * nBatches # Initialize batchSizes list
if nTrain % batchSize != 0: # Adjust last batch size if there's a remainder
    batchSizes[-1] = nTrain % batchSize

batchIndex = [0] + list(np.cumsum(batchSizes)) # Corrected batchIndex calculation


epoch = 0

lossTrain = dict()
lossValid = dict()
costTrain = dict()
costValid = dict()

lossTestBest = dict()
costTestBest = dict()
lossTestLast = dict()
costTestLast = dict()

bestModel = dict()
for key in architectures.keys():
  lossTrain[key] = []
  lossValid[key] = []
  costTrain[key] = []
  costValid[key] = []
  bestModel[key] = None # Initialize bestModel for each architecture
while epoch < nEpochs :
  randomPermutation = np.random.permutation(nTrain)
  idxEpoch = [int(i) for i in randomPermutation]
  batch = 0
  while batch < nBatches:
    thisBatchIndices = idxEpoch[batchIndex[batch]:batchIndex[batch+1]]
    xTrainBatch = xTrain[thisBatchIndices,:,:]
    yTrainBatch = yTrain[thisBatchIndices,:,:]
    for key in architectures.keys():
      architectures[key].zero_grad()
      yHatTrainBatch = architectures[key](xTrainBatch)
      lossValueTrain = loss(yHatTrainBatch.squeeze(),yTrainBatch.squeeze())
      lossValueTrain.backward()
      optimizers[key].step()
      costValueTrain = lossValueTrain.item()
      lossTrain[key] += [lossValueTrain.item()]
      costTrain[key] += [costValueTrain]
      if (epoch * nBatches + batch) % validationInterval == 0:
        with torch.no_grad():
          yHatValid = architectures[key](xValid)
        lossValueValid = loss(yHatValid.squeeze(),yValid.squeeze())
        costValueValid = lossValueValid.item()
        lossValid[key] += [lossValueValid.item()]
        costValid[key] += [costValueValid]
        if bestModel[key] is None or costValueValid <= min(costValid[key]): # Corrected condition for saving best model
          bestModel[key] = copy.deepcopy(architectures[key].state_dict()) # Save state_dict

    batch +=1
  epoch +=1

#### Evaluation ####
for key in architectures.keys():
  with torch.no_grad():
    yHatTest = architectures[key](xTest)
  lossTestLast[key] = loss(yHatTest.squeeze(),yTest.squeeze())
  costTestLast[key] = lossTestLast[key].item()
  with torch.no_grad():
    architectures[key].load_state_dict(bestModel[key])
    yHatTest = architectures[key](xTest)
  lossTestBest[key] = loss(yHatTest.squeeze(),yTest.squeeze())
  costTestBest[key] = lossTestBest[key].item()

import matplotlib.pyplot as plt

for key in architectures.keys():
  fig,ax = plt.subplots(figsize=(10,6))
  ax.plot(lossTrain[key],linestyle='--')
  ax.plot(lossValid[key],marker='^')
  ax.set_title(key,fontweight='bold')
  plt.legend()
  plt.grid()
  plt.show()

#### Transferability -> 1 ####
N2= 500
S2 = sbm(n=N2)
S2 = normalize_gso(S2)
z2 = generate_diffusion(gso=S2,n_samples=nTest,n_sources=100)
x2,y2 = data_from_diffusion(z2)
xTest2 = x2
yTest2 = y2

xTest2 = torch.tensor(xTest2).reshape([-1,1,N2])
yTest2 = torch.tensor(yTest2).reshape([-1,1,N2])
lossTest2 = dict()
costTest2 = dict()

# Create new instances of GNN models and load the best state dictionaries
gnn1_best = GNN(S, 2, [8, 1], [1, 32, 1], nn.ReLU(), True)
gnn1_best.load_state_dict(bestModel['GNN 1 layer'])

gnn2_best = GNN(S, 3, [5, 5, 1], [1, 16, 4, 1], nn.ReLU(), True)
gnn2_best.load_state_dict(bestModel['GNN 2 layer'])

# Change the GSO for the new model instances
gnn1_best.changeGSO(S2)
gnn2_best.changeGSO(S2)


with torch.no_grad():
  yHatTest2 = gnn1_best(xTest2)
lossTest2['GNN 1 layer'] = loss(yHatTest2.squeeze(),yTest2.squeeze())
costTest2['GNN 1 layer'] = lossTest2['GNN 1 layer'].item()
with torch.no_grad():
  yHatTest2 = gnn2_best(xTest2)
lossTest2['GNN 2 layer'] = loss(yHatTest2.squeeze(),yTest2.squeeze())
costTest2['GNN 2 layer'] = lossTest2['GNN 2 layer'].item()


print(" " + "GNN 1 layer: %6.4f" % (costTest2['GNN 1 layer']))
print(" " + "GNN 2 layer: %6.4f" % (costTest2['GNN 2 layer']))

#### Transferability -> 2 ####

N3 = 1000
S3 = sbm(n=N3)
S3 = normalize_gso(S3)

z3 = generate_diffusion(gso=S3,n_samples=nTest,n_sources = 200)

x3,y3 = data_from_diffusion(z3)

xTest3 = x3
yTest3 = y3

xTest3 = torch.tensor(xTest3).reshape([-1,1,N3])
yTest3 = torch.tensor(yTest3).reshape([-1,1,N3])
lossTest3 = dict()
costTest3 = dict()

# Create new instances of GNN models and load the best state dictionaries
gnn1_best_N3 = GNN(S, 2, [8, 1], [1, 32, 1], nn.ReLU(), True)
gnn1_best_N3.load_state_dict(bestModel['GNN 1 layer'])

gnn2_best_N3 = GNN(S, 3, [5, 5, 1], [1, 16, 4, 1], nn.ReLU(), True)
gnn2_best_N3.load_state_dict(bestModel['GNN 2 layer'])

# Change the GSO for the new model instances
gnn1_best_N3.changeGSO(S3)
gnn2_best_N3.changeGSO(S3)


with torch.no_grad():
  yHatTest3 = gnn1_best_N3(xTest3)
lossTest3['GNN 1 layer'] = loss(yHatTest3.squeeze(),yTest3.squeeze())
costTest3['GNN 1 layer'] = lossTest3['GNN 1 layer'].item()

with torch.no_grad():
  yHatTest3 = gnn2_best_N3(xTest3)
lossTest3['GNN 2 layer'] = loss(yHatTest3.squeeze(),yTest3.squeeze())
costTest3['GNN 2 layer'] = lossTest3['GNN 2 layer'].item()

print(" " + "GNN 1 layer: %6.4f" % (costTest3['GNN 1 layer']))
print(" " + "GNN 2 layer: %6.4f" % (costTest3['GNN 2 layer']))

"""**Filter trial**"""

import numpy as np
import torch # Import torch

def sbm(n=50, c=5, p_intra = 0.6, p_inter = 0.2):
  community = np.repeat(list(range(c)),np.ceil(n/c))
  community = community[0:n]
  community = np.expand_dims(community,1)
  intra = community == community.T
  inter = np.logical_not(intra)
  random = np.random.random((n,n))
  tri = np.tri(n,k=-1)
  graph = np.zeros((n,n))
  graph[np.logical_and.reduce([tri, intra, random < p_inter])] = 1
  graph[np.logical_and.reduce([tri, inter, random < p_inter])] = 1
  graph += graph.T
  return graph
def normalize_gso(gso):
  eig_val,eig_vec = np.linalg.eig(gso)
  return gso/np.max(eig_val.real)
def generate_diffusion(gso, n_samples=2100, n_sources=10):
  n = gso.shape[0] ##number of nodes
  # Convert gso to torch tensor if it's not already
  if not isinstance(gso, torch.Tensor):
      gso = torch.tensor(gso, dtype=torch.complex128 if gso.dtype == np.complex128 else torch.float64) # Handle complex GSO

  z = torch.zeros((n_samples, n, 11, 1), dtype=torch.complex128 if gso.dtype == torch.complex128 else torch.float64) # Initialize z with complex dtype if gso is complex
  for i in range(n_samples):
    sources = np.random.choice(n, n_sources, replace=False)
    z[i,sources,0,0] = torch.rand(n_sources, dtype=torch.complex128 if gso.dtype == torch.complex128 else torch.float64) * 10 # Use torch.rand and handle complex dtype

  mu = torch.zeros(n, dtype=torch.complex128 if gso.dtype == torch.complex128 else torch.float64) # Use torch.zeros and handle complex dtype
  sigma = torch.eye(n, dtype=torch.complex128 if gso.dtype == torch.complex128 else torch.float64) * 1e-3 # Use torch.eye and handle complex dtype

  for t in range(10):
    noise = torch.distributions.multivariate_normal.MultivariateNormal(mu.real, sigma.real).sample((n_samples,)) # Use torch distributions for noise (real part for now)

    z[:,:,t+1] = torch.matmul(gso, z[:,:,t]) + noise.unsqueeze(-1)

  z = z.transpose(1,2)
  return z.squeeze()
def data_from_diffusion(z):
  z = z[torch.randperm(z.shape[0])]
  y = z[:,0,:].unsqueeze(1)
  x = torch.zeros_like(y)
  x = z[:,4,:]
  return x.squeeze(),y.squeeze()
def split_data(x,y, splits=(2000,100)):
  splits = np.cumsum([0] + list(splits))
  splits = (splits * x.shape[0]/splits[-1]).astype(int) ## to avoid blowing up
  return ((x[splits[i] : splits[i+1]],y[splits[i]:splits[i+1]]) for i in range(len(splits)-1))

import torch
import torch.nn as nn
import math

def FilterFunction(h, S, x, b=None):
  F = h.shape[0]
  K = h.shape[1] #filter taps
  G = h.shape[2]

  N = S.shape[1]
  B = x.shape[0]
  x = x.reshape([B, 1, G, N])
  S = S.reshape([1, N, N])
  z = x
  for k in range(1,K):
    x = torch.matmul(x,S)
    xS = x.reshape([B, 1, G, N])
    z = torch.cat((z,xS), dim = 1)
  y = torch.matmul(z.permute([0,3,1,2]).reshape([B,N,K*G]) , h.reshape([F,K*G]).permute([1,0])).permute([0,2,1])
  if b is not None:
    y = y + b ## add a bias learning parameter
  return y
class GraphFilterFunction(nn.Module):
  def __init__(self,gso,k,f_in,f_out,bias):
    super().__init__() # Added missing super().__init__()
    self.gso = torch.tensor(gso)
    self.n = gso.shape[0]
    self.f_in = f_in
    self.f_out = f_out
    self.k = k
    if bias:f
      self.bias = nn.parameter.Parameter(torch.randn(self.f_out, 1)) # bias initialization
    self.weight = nn.Parameter(torch.randn(self.f_out,self.k,self.f_in))
    self.reset_parameter()
  def reset_parameter(self):
    stdv = 1./math.sqrt(self.f_in * self.k)
    self.weight.data.uniform_(-stdv,stdv)
    if self.bias is not None:
      self.bias.data.uniform_(-stdv,stdv)
  def forward(self,x):
    return FilterFunction(self.weight ,self.gso ,x ,self.bias)
  def changeGSO(self,new_gso):
    self.gso = torch.tensor(new_gso)
    self.n = new_gso.shape[0]
class GNN(nn.Module):
  def __init__(self,gso,l,k,f,sigma,bias):
    super().__init__()
    self.gso = torch.tensor(gso)
    self.n = gso.shape[0]
    self.l = l
    self.k = k
    self.f = f
    self.sigma = sigma
    self.bias = bias
    gml = []
    for layer in range(l):
      gml.append(GraphFilterFunction(gso,k[layer],f[layer],f[layer+1],bias))
      gml.append(sigma)
    self.gml = nn.Sequential(*gml)
  def forward(self,x):
    return self.gml(x)
  def changeGSO(self, new_gso):
    self.gso = new_gso
    for layer in range(2*self.l):
      if layer %  2 == 0:
        self.gml[layer].changeGSO(new_gso)
    self.n = new_gso.shape[0]

import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim
import torch.nn as nn
import copy
torch.set_default_dtype(torch.float64)

N = 50 #number of nodes
S = sbm(n=N)
S = normalize_gso(S)

nTrain = 2000
nTest = 100

z = generate_diffusion(gso=S,n_samples=nTrain+nTest)
x,y = data_from_diffusion(z)

trainData,testData = split_data(x,y,(nTrain,nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]

xTrain = torch.tensor(xTrain).reshape([-1,1,N])
yTrain = torch.tensor(yTrain).reshape([-1,1,N])
xTest = torch.tensor(xTest).reshape([-1,1,N])
yTest = torch.tensor(yTest).reshape([-1,1,N])

loss = nn.MSELoss()
architectures = dict()
## linear parameterization
linearParam = torch.nn.Linear(N,N,bias=True)
architectures['LinearLayer'] = linearParam
## Fully Connected Neural Network
fcnet = nn.Sequential(torch.nn.Linear(N,25,bias=True),nn.ReLU(),torch.nn.Linear(25,N,bias=True),nn.ReLU())
architectures['FCNN'] = fcnet
## Multi-feature Graph Filter
MLgraphFilter = nn.Sequential(GraphFilterFunction(S,8,1,32,True),GraphFilterFunction(S,1,32,1,True))
architectures['MLGraphFilter'] = MLgraphFilter
## GNN 1 layer
GNN1Ly = GNN(S,2,[8,1],[1,32,1],nn.ReLU(),True)
architectures['GNN 1 layer'] = GNN1Ly
## GNN 2 layer
GNN2Ly = GNN(S,3,[5,5,1],[1,16,4,1],nn.ReLU(),True)
architectures['GNN 2 layer'] = GNN2Ly

### Training ###
validationInterval = 5
nEpochs = 30
batchSize = 200
learningRate = 0.05

nValid = int(np.floor(0.01*nTrain))
xValid = xTrain[0:nValid,:,:]
yValid = yTrain[0:nValid,:,:]
xTrain = xTrain[nValid:,:,:]
yTrain = yTrain[nValid:,:,:]
nTrain = xTrain.shape[0]

optimizers = dict()
for key in architectures.keys():
  optimizers[key] = optim.Adam(architectures[key].parameters(), lr = learningRate)
if nTrain < batchSize :
  nBatches = 1
  batchSize = [nTrain]
elif nTrain % batchSize != 0:
  nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches
  while sum(batchSize) != nTrain :
    batchSize[-1] -= 1
else:
  nBatches = np.int(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches

batchIndex = [0] + list(np.cumsum(batchSize).tolist())
epoch = 0
## stroing the training
lossTrain = dict()
lossValid = dict()
costTrain = dict()
costValid = dict()
## test variables

lossTestBest = dict()
costTestBest = dict()
lossTestLast = dict()
costTestLast = dict()

bestModel = dict()
for key in architectures.keys():
  lossTrain[key] = []
  lossValid[key] = []
  costTrain[key] = []
  costValid[key] = []
  bestModel[key] = None
while epoch < nEpochs:
  randomPermutation = np.random.permutation(nTrain)
  idxEpoch = [int(i) for i in randomPermutation]
  batch = 0
  while batch < nBatches:
    thisBatchIndices = idxEpoch[batchIndex[batch]:batchIndex[batch + 1]]
    xTrainBatch = xTrain[thisBatchIndices,:,:]
    yTrainBatch = yTrain[thisBatchIndices,:,:]
    for key in architectures.keys():
      architectures[key].zero_grad()
      yHatTrainBatch = architectures[key](xTrainBatch)
      lossValueTrain = loss(yHatTrainBatch.squeeze(),yTrainBatch.squeeze())
      lossValueTrain.backward()
      optimizers[key].step()
      costValueTrain = lossValueTrain.item()
      lossTrain[key] += [lossValueTrain.item()]
      costTrain[key] += [costValueTrain]
      if (epoch * nBatches + batch) % validationInterval == 0:
        with torch.no_grad(): ### GNN Output
          yHatValid = architectures[key](xValid)
        lossValueValid = loss(yHatValid.squeeze(), yValid.squeeze())
        costValueValid = lossValueValid.item()
        lossValid[key] += [lossValueValid.item()]
        costValid[key] += [costValueValid]
        if bestModel[key] is None or costValueValid <= min(costValid[key]): # Save state_dict
            bestModel[key] = copy.deepcopy(architectures[key].state_dict())
    batch +=1
  epoch+=1

for key in architectures.keys(): # Changed .items() to .keys()
  plt.plot(lossValid[key],label='lossValid')
  plt.plot(lossTrain[key],label = 'lossTrain')
  plt.legend()
  plt.grid()
  plt.title(key)
  plt.show()

#### Evaluation ####
for key in architectures.keys():
  with torch.no_grad():
    yHatTest = architectures[key](xTest)
  lossTestLast[key] = loss(yHatTest.squeeze(),yTest.squeeze())
  costTestLast[key] = lossTestLast[key].item()
  with torch.no_grad():
    architectures[key].load_state_dict(bestModel[key]) # Load state_dict
    yHatTest = architectures[key](xTest)
  lossTestBest[key] = loss(yHatTest.squeeze(),yTest.squeeze())
  costTestBest[key] = lossTestBest[key].item()

### working on transferability ###

N = 1000
S = sbm(n=N)
S = normalize_gso(S)

nTrain = 2000
nTest = 100
z = generate_diffusion(gso=S,n_samples=nTrain+nTest)
x,y = data_from_diffusion(z)
xTest,yTest = data_from_diffusion(z) # This line generates new test data, might not be intended

xTest = torch.tensor(xTest).reshape([-1,1,N])
yTest = torch.tensor(yTest).reshape([-1,1,N])
lossTest = dict()
costTest = dict()

# Create new instances of GNN models and load the best state dictionaries
gnn1_transfer = GNN(S, 2, [8, 1], [1, 32, 1], nn.ReLU(), True)
gnn1_transfer.load_state_dict(bestModel['GNN 1 layer'])

gnn2_transfer = GNN(S, 3, [5, 5, 1], [1, 16, 4, 1], nn.ReLU(), True)
gnn2_transfer.load_state_dict(bestModel['GNN 2 layer'])

# Change the GSO for the new model instances
gnn1_transfer.changeGSO(S)
gnn2_transfer.changeGSO(S)

with torch.no_grad():
  yHatTest = gnn1_transfer(xTest)
lossTest['GNN 1 layer'] = loss(yHatTest.squeeze(),yTest.squeeze())
costTest['GNN 1 layer'] = lossTest['GNN 1 layer'].item()
with torch.no_grad():
  yHatTest = gnn2_transfer(xTest)
lossTest['GNN 2 layer'] = loss(yHatTest.squeeze(),yTest.squeeze())
costTest['GNN 2 layer'] = lossTest['GNN 2 layer'].item()


print(" " + "GNN 1 layer: %6.4f" % (costTest['GNN 1 layer']))
print(" " + "GNN 2 layer: %6.4f" % (costTest['GNN 2 layer']))

"""#**4 Node Closed**"""

import numpy as np
import matplotlib.pyplot as plt
import torch.optim as optim
import torch.nn as nn
import copy
torch.set_default_dtype(torch.float64)

N = 4
S = np.zeros((N,N))
for i in range(N):
  S[i , (i+1)%N] = 1
  S[(i+1)%N, i ] = 1
S = torch.tensor(S)
S = normalize_gso(S)
nTrain = 2000
nTest = 100
z = generate_diffusion(gso=S,n_samples=nTrain+nTest, n_sources=N)
x,y = data_from_diffusion(z)
trainData,testData = split_data(x,y,(nTrain,nTest))
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]
xTrain = torch.tensor(xTrain).reshape([-1,1,N])
yTrain = torch.tensor(yTrain).reshape([-1,1,N])
xTest  = torch.tensor(xTest).reshape([-1,1,N])
yTest  = torch.tensor(yTest).reshape([-1,1,N])

loss = nn.MSELoss()

architectures = dict()
linearParam = torch.nn.Linear(N,N,bias=True)
architectures['LinearLayer'] = linearParam
fcnet = nn.Sequential(torch.nn.Linear(N,25,bias=True),nn.ReLU(),torch.nn.Linear(25,N,bias=True),nn.ReLU())
architectures['FCNN'] = fcnet
MLgraphFilter = nn.Sequential(GraphFilterFunction(S,8,1,32,True),GraphFilterFunction(S,1,32,1,True))
architectures['MLGraphFilter'] = MLgraphFilter
GNN1Ly = GNN(S,2,[8,1],[1,32,1],nn.ReLU(),True)
architectures['GNN 1 layer'] = GNN1Ly
GNN2Ly = GNN(S,3,[5,5,1],[1,16,4,1],nn.ReLU(),True)
architectures['GNN 2 layer'] = GNN2Ly

### Training ###
validationInterval = 5
nEpochs = 30
batchSize = 200
learningRate = 0.05

nValid = int(np.floor(0.01*nTrain))
xValid = xTrain[0:nValid,:,:]
yValid = yTrain[0:nValid,:,:]
xTrain = xTrain[nValid:,:,:]
yTrain = yTrain[nValid:,:,:]
nTrain = xTrain.shape[0]

optimizers = dict()
for key in architectures.keys():
  optimizers[key] = optim.Adam(architectures[key].parameters(), lr = learningRate)
if nTrain < batchSize :
  nBatches = 1
  batchSize = [nTrain]
elif nTrain % batchSize != 0:
  nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches
  while sum(batchSize) != nTrain :
    batchSize[-1] -= 1
else:
  nBatches = np.int(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches

batchIndex = [0] + list(np.cumsum(batchSize).tolist())
epoch = 0
## stroing the training
lossTrain = dict()
lossValid = dict()
costTrain = dict()
costValid = dict()
## test variables

lossTestBest = dict()
costTestBest = dict()
lossTestLast = dict()
costTestLast = dict()

bestModel = dict()
for key in architectures.keys():
  lossTrain[key] = []
  lossValid[key] = []
  costTrain[key] = []
  costValid[key] = []
  bestModel[key] = None
while epoch < nEpochs:
  randomPermutation = np.random.permutation(nTrain)
  idxEpoch = [int(i) for i in randomPermutation]
  batch = 0
  while batch < nBatches:
    thisBatchIndices = idxEpoch[batchIndex[batch]:batchIndex[batch + 1]]
    xTrainBatch = xTrain[thisBatchIndices,:,:]
    yTrainBatch = yTrain[thisBatchIndices,:,:]
    for key in architectures.keys():
      architectures[key].zero_grad()
      yHatTrainBatch = architectures[key](xTrainBatch)
      lossValueTrain = loss(yHatTrainBatch.squeeze(),yTrainBatch.squeeze())
      lossValueTrain.backward()
      optimizers[key].step()
      costValueTrain = lossValueTrain.item()
      lossTrain[key] += [lossValueTrain.item()]
      costTrain[key] += [costValueTrain]
      if (epoch * nBatches + batch) % validationInterval == 0:
        with torch.no_grad():
          yHatValid = architectures[key](xValid)
        lossValueValid = loss(yHatValid.squeeze(), yValid.squeeze())
        costValueValid = lossValueValid.item()
        lossValid[key] += [lossValueValid.item()]
        costValid[key] += [costValueValid]
        if bestModel[key] is None or costValueValid <= min(costValid[key]):
            bestModel[key] = copy.deepcopy(architectures[key].state_dict())
    batch +=1
  epoch+=1

fig, axes = plt.subplots(5, 1, figsize=(10, 10))
axes = axes.flatten()
color = np.array(['red','blue','green','purple','teal'])
for i, key in enumerate(architectures.keys()):
    if i < len(axes):
        axes[i].plot(lossTrain[key],c=color[i] ,label='lossTrain')
        axes[i].legend()
        axes[i].grid()
        axes[i].set_title(key,fontweight='bold')
    else:
        print(f"Warning: More architectures ({len(architectures)}) than subplots ({len(axes)}). Skipping plot for {key}")
plt.savefig('lossTrain.png',dpi=300)
plt.tight_layout() # Adjust layout to prevent overlapping titles/labels
plt.show()

fig,axes = plt.subplots(5,1,figsize=(10,8))
axes = axes.flatten()
color = ['magenta','cyan','coral','teal','yellow']
for i,num in enumerate(architectures.keys()):
  if i < len(axes):
    # axes[i].set_facecolor('aqua')
    axes[i].plot(lossValid[num],c=color[i],label='lossValid')
    axes[i].legend()
    axes[i].grid()
    axes[i].set_title(num,fontweight='bold')
plt.tight_layout()
plt.savefig('lossValid.png',dpi=300)
plt.show()

"""#**Data Generation_Star Network(*Oscillator Trial*)**"""

import numpy as np
import torch
import torch.nn as nn

def sbm(n = 50, c = 5, p_intra = 0.6 , p_inter = 0.2):
    community = np.repeat(list(range(c)),np.ceil(n/c))
    community = community[0:n]
    community = np.expand_dims(community,1)
    random = np.random.random((n,n))
    intra = community == community.T
    inter = np.logical_not(intra)
    tri = np.tri(n,k=-1)
    graph = np.zeros((n,n))
    graph[np.logical_and.reduce([tri,intra,random<p_intra])] = 1
    graph[np.logical_and.reduce([tri,inter,random<p_inter])] = 1
    graph += graph.T
    return graph

def normalize_gso(gso_tensor):
    if gso_tensor.dim() == 2:
        gso_tensor_4d = gso_tensor.unsqueeze(0).unsqueeze(0)
    else:
        gso_tensor_4d = gso_tensor

    num_features = gso_tensor_4d.shape[1]
    bn_layer = nn.BatchNorm2d(num_features=num_features)
    bn_layer.eval()
    normalized_gso_4d = bn_layer(gso_tensor_4d)

    if gso_tensor.dim() == 2:
        return normalized_gso_4d.squeeze(0).squeeze(0)
    else:
        return normalized_gso_4d

def generate_diffusion(gso, n_samples=2000, n_sources=10):
    n = gso.shape[0]
    if not isinstance(gso, torch.Tensor):
        gso = torch.tensor(gso, dtype=torch.complex128 if gso.dtype == np.complex128 else torch.float64)

    is_complex = gso.is_complex()
    dtype = torch.complex128 if is_complex else torch.float64

    z = torch.zeros((n_samples, 11, n), dtype=dtype)

    for i in range(n_samples):
        sources = np.random.choice(n, n_sources, replace=False)
        z[i, 0, sources] = torch.rand(n_sources, dtype=dtype)

    mu = torch.zeros(n, dtype=dtype)
    sigma = torch.ones(n, dtype=dtype) * 1e-3

    if is_complex:
        noise_dist = torch.distributions.multivariate_normal.MultivariateNormal(
            mu.real, torch.diag(sigma.real)
        )
    else:
        noise_dist = torch.distributions.multivariate_normal.MultivariateNormal(
            mu, torch.diag(sigma)
        )

    for t in range(10):
        noise = noise_dist.sample((n_samples,))
        z[:, t + 1, :] = torch.matmul(z[:, t, :], gso.T) + noise

    return z.squeeze().detach().numpy()
def data_from_diffusion(z):
  z = z[np.random.permutation(z.shape[0])]
  y = z[:,0,:]
  y = np.expand_dims(y,axis=1)
  x = z[:,4,:]
  return x.squeeze(),  y.squeeze()
def split_data(x,y,splits=(2000,100)):
  splits = np.cumsum( [0] + list(splits))
  splits = (splits * x.shape[0]/splits[-1]).astype(int)
  return ((x[splits[i] : splits[i + 1],: ] , y[splits[i] : splits[i + 1],: ]) for i in range(len(splits) - 1))

"""#**Filter Function and GNN Module(*Oscillator trial*)**"""

import math
def FilterFunction(h ,S ,x , b=None):
  F = h.shape[0]
  K = h.shape[1]
  G = h.shape[2]
  N = S.shape[1]
  B = x.shape[0]
  x = x.reshape([B,1,G,N])
  S = S.reshape([1,N,N])
  z = x
  for k in range(1,K):
    x = torch.matmul(x,S)
    xS = x.reshape([B,1,G,N])
    z = torch.cat((z,xS),dim=1)
  y = torch.matmul(z.permute([0,3,1,2]).reshape([B,N,K*G]), h.reshape([F,K*G]).permute([1,0])).permute([0,2,1])
  if b is not None:
    y = y + b
  return y
class GraphFilterFunction(nn.Module):
  def __init__(self,gso,k,f_in,f_out,bias):
    super().__init__()
    self.gso = gso.clone().detach()
    self.n = gso.shape[0]
    self.f_in = f_in
    self.f_out = f_out
    self.k = k
    if bias:
      self.bias = nn.parameter.Parameter(torch.zeros(self.f_out,1))
    else:
      self.register_parameter('bias',None)
    self.weight = nn.Parameter(torch.randn(self.f_out,self.k,self.f_in))
    self.reset_parameters()
  def reset_parameters(self):
    stdv = 1./math.sqrt(self.f_in*self.k)
    self.weight.data.uniform_(-stdv,stdv)
    if self.bias is not None:
      self.bias.data.uniform_(-stdv,stdv)
  def forward(self,x):
    return FilterFunction(self.weight,self.gso,x,self.bias)
  def changeGSO(self,new_gso):
    self.gso = torch.tensor(new_gso)
    self.n = new_gso.shape[0]
class GNN(nn.Module):
  def __init__(self,gso,l,k,f,sigma,bias):
    super().__init__()
    self.gso = gso.clone().detach()
    self.n = gso.shape[0]
    self.l = l
    self.k = k
    self.f = f
    self.sigma = sigma
    self.bias = bias
    gml = []
    for layer in range(l):
      gml.append(GraphFilterFunction(gso,k[layer],f[layer],f[layer + 1],bias))
      gml.append(sigma)
    self.gml = nn.Sequential(*gml)
  def forward(self,x):
    return self.gml(x)
  def changeGSO(self,new_gso):
    self.gso = gso.clone().detach()
    for layer in range(2*self.l):
      if layer % 2 == 0:
        self.gml[layer].changeGSO(new_gso)
    self.n = new_gso.shape[0]

"""#**Execution Block(*Oscillator Trial*)**"""

torch.set_default_dtype(torch.float64)
import torch.optim as optim
import copy

N = 4
a = np.zeros((N, N), dtype=complex)
a_herm = np.zeros((N, N), dtype=complex)
for i in range(1, N):
    a[i, i - 1] = np.sqrt(i)
    a_herm[i - 1, i] = np.sqrt(i)

p_op = 1j * (a_herm - a)
x_op = (a_herm + a)
P = np.matmul(p_op, p_op)
X = np.matmul(x_op, x_op)
S_part = P + X

nTrain = 2000
nTest = 100

S_part_torch = torch.from_numpy(S_part.real)
S = normalize_gso(S_part_torch)

print("Original tensor data type:", S_part_torch.dtype)
print("Normalized tensor data type:", S.dtype)
print("Normalized tensor shape:", S.shape)

S_norm = S.detach().numpy()
z = generate_diffusion(gso=S, n_samples=nTrain + nTest, n_sources=N)
print("\nShape of the generated diffusion tensor 'z':", z.shape)
x,y = data_from_diffusion(z)
print("\nShape of the input data 'x':", x.shape)
print("Shape of the output data 'y':", y.shape)
trainData,testData = split_data(x,y,)
xTrain = trainData[0]
yTrain = trainData[1]
xTest = testData[0]
yTest = testData[1]
xTrain = torch.tensor(xTrain).reshape([-1,1,N])
yTrain = torch.tensor(yTrain).reshape([-1,1,N])
xTest  = torch.tensor(xTest).reshape([-1,1,N])
yTest  = torch.tensor(yTest).reshape([-1,1,N])
loss = nn.MSELoss()
architectures = dict()
linearParam = torch.nn.Linear(N,N,bias=True)
architectures['linear'] = linearParam
fcnet = nn.Sequential(torch.nn.Linear(N,25,bias=True),nn.ReLU(),torch.nn.Linear(25,N,bias=True),nn.ReLU())
architectures['fcnet'] = fcnet
MLGraphFilter = nn.Sequential(GraphFilterFunction(S,8,1,32,True),GraphFilterFunction(S,8,32,1,True))
architectures['MLGraphFilter'] = MLGraphFilter
GNN1Ly = GNN(S,2,[8,1],[1,32,1],nn.ReLU(),True)
architectures['GNN 1 Layer'] = GNN1Ly
GNN2Ly = GNN(S,3,[5,5,1],[1,16,4,1],nn.ReLU(),True)
architectures['GNN 2 Layer'] = GNN2Ly

### Training ###
validationInterval = 5
nEpochs = 30
batchSize = 200
learningRate = 0.05

nValid = int(np.floor(0.01*nTrain))
xValid = xTrain[0:nValid,:,:]
yValid = yTrain[0:nValid,:,:]
xTrain = xTrain[nValid:,:,:]
yTrain = yTrain[nValid:,:,:]
nTrain = xTrain.shape[0]
optimizers = dict()
for key in architectures.keys():
  optimizers[key] = optim.Adam(architectures[key].parameters(),lr=learningRate)
if nTrain < batchSize:
  nBatches = 1
  batchSize = [nTrain]
elif nTrain % batchSize != 0 :
  nBatches = np.ceil(nTrain/batchSize).astype(np.int64)
  batchSize = [batchSize] * nBatches
  batchSize[-1] = nTrain - sum(batchSize[:-1])
  while sum(batchSize) != nTrain:
    batchSize[-1] -= 1
else:
  nBatches = np.int(nTrain/bacthSize).astype(np.int64)
  batchSize = [batchSize] * nBatches

batchIndex = [0] + list(np.cumsum(batchSize).tolist())
epoch = 0

### Strorage of Training ###
lossTrain = dict()
lossValid = dict()
costTrain = dict()
costValid = dict()
### test variables ###
lossTestBest = dict()
costTestBest = dict()
lossTestLast = dict()
costTestLast = dict()

bestModel = dict()
for key in architectures.keys():
  lossTrain[key] = []
  lossValid[key] = []
  costTrain[key] = []
  costValid[key] = []
  bestModel[key] = copy.deepcopy(architectures[key].state_dict())
while epoch < nEpochs:
  randomPermutation = np.random.permutation(nTrain)
  idxEpoch = [int(i) for i in randomPermutation]
  batch = 0
  while batch < nBatches:
      thisBatchIndices = idxEpoch[batchIndex[batch] : batchIndex[batch + 1]]
      xTrainBatch = xTrain[thisBatchIndices,:,:]
      yTrainBatch = yTrain[thisBatchIndices,:,:]

      for key in architectures.keys():
          architectures[key].train()
          architectures[key].zero_grad()
          yHatTrainBatch = architectures[key](xTrainBatch)
          lossValueTrain = loss(yHatTrainBatch.squeeze(),yTrainBatch.squeeze())
          lossValueTrain.backward()
          optimizers[key].step()
          costValueTrain = lossValueTrain.item()
          lossTrain[key] += [lossValueTrain.item()]
          costTrain[key] += [costValueTrain]

          if (epoch * nBatches + batch) % validationInterval == 0:
              with torch.no_grad():
                yHatValid = architectures[key](xValid)
              lossValueValid = loss(yHatValid.squeeze(), yValid.squeeze())
              costValueValid = lossValueValid.item()
              lossValid[key] += [lossValueValid.item()]
              costValid[key] += [costValueValid]

              if len(costValid[key]) > 1:
                if costValueValid <= min(costValid[key]):
                  bestModel[key] = copy.deepcopy(architectures[key].state_dict())
                else:
                  bestModel[key] = copy.deepcopy(architectures[key].state_dict())
      batch += 1
  epoch += 1

"""#**Model Weight Extraction for two layers**"""

GNN_model_l1 = architectures['GNN 1 Layer']
model_wt_GNN = GNN_model_l1.state_dict()
for param_name,param_tensor in model_wt_GNN.items():
  print(f"Parameter '{param_name}': shape {param_tensor.shape}")
first_layer_weights = model_wt_GNN['gml.0.weight']
second_layer_weights = model_wt_GNN['gml.2.weight']
print("\nWeights of the first GraphFilterFunction layer:")
print(first_layer_weights)
print("\nWeights of the second GraphFilterFunction layer:")
print(second_layer_weights)

import matplotlib.pyplot as plt
first_layer_weights.shape

plt.figure(figsize=(10, 6))
for i in range(first_layer_weights.shape[1]):
    plt.plot(first_layer_weights.detach().numpy()[:, i], label=f'Output Feature {i}')

plt.xlabel('Input Feature Index')
plt.ylabel('Weight Value')
plt.title('Weights of the First GNN Layer 2')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 8))
first_layer_weights = first_layer_weights.reshape([32,8])
heatmap_data = first_layer_weights.detach().numpy()

plt.pcolormesh(heatmap_data, cmap='inferno')
plt.colorbar(label='Weight Value')
plt.xlabel('Output Feature Index')
plt.ylabel('Input Feature Index')
plt.title('Heatmap of the First GNN layer 2')
plt.savefig('GNN_layer_2.png',dpi=300)
plt.gca().invert_yaxis()
plt.show()

ml_graph_filter_model = architectures['MLGraphFilter']
model_weights = ml_graph_filter_model.state_dict()

print("Model parameters in the state_dict:")
for param_name, param_tensor in model_weights.items():
    print(f"  - Parameter '{param_name}': shape {param_tensor.shape}")

first_layer_weights = model_weights['0.weight']
print("\nWeights of the first GraphFilterFunction layer:")
print(first_layer_weights)
second_layer_weights = model_weights['1.weight']
print("\nWeights of the second GraphFilterFunction layer:")
print(second_layer_weights)

plt.figure(figsize=(10, 6))
for i in range(first_layer_weights.shape[1]):
    plt.plot(first_layer_weights.detach().numpy()[:, i], label=f'Output Feature {i}')

plt.xlabel('Input Feature Index')
plt.ylabel('Weight Value')
plt.title('Weights of the First GraphFilterFunction Layer')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10, 8))
first_layer_weights = first_layer_weights.reshape([32,8])
heatmap_data = first_layer_weights.detach().numpy()

plt.pcolormesh(heatmap_data, cmap='cubehelix_r')
plt.colorbar(label='Weight Value')
plt.xlabel('Output Feature Index')
plt.ylabel('Input Feature Index')
plt.title('Heatmap of the First GraphFilterFunction Layer Weights')
plt.savefig("GraphFilter_first_layer_weights.png",dpi=300)
plt.gca().invert_yaxis()
plt.show()

fig, axes = plt.subplots(5,1, figsize=(10,10))
axes = axes.flatten()
color = np.array(['red','blue','green','purple','teal'])
for i,key in enumerate(architectures.keys()):
  if i < len(axes):
    axes[i].plot(lossTrain[key],c=color[i],label='lossTrain')
    axes[i].legend()
    axes[i].grid()
    axes[i].set_title(key,fontweight='bold')
# plt.savefig('loss_train_star_net(1Doscillator).png',dpi=300)
plt.tight_layout()
plt.show()

fig, axes = plt.subplots(5,1, figsize=(10,10))
axes = axes.flatten()
color = np.array(['red','blue','green','purple','teal'])
for i,key in enumerate(architectures.keys()):
  if i < len(axes):
    axes[i].plot(lossValid[key],c=color[i],label='lossValid')
    axes[i].legend()
    axes[i].grid()
    axes[i].set_title(key,fontweight='bold')
# plt.savefig('loss_valid_star_net(1Doscillator).png',dpi=300)
plt.tight_layout()
plt.show()